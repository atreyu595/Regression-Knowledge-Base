{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a3c170-4b0d-49d6-8ba1-3a0f06adb585",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1> Regression Notes </h1>\n",
    "\n",
    "- Regression can be simply  computed by taking the weighted sum of input features plus a constant called the \"bias term\"\n",
    "- Common way to measure it is using RMSE (Root Mean Squared Error)\n",
    "- To minimize RMSE, we need to find the values theta (coefficient of input features).\n",
    "- We want to minimise the MSE which can be easier than RMSE.\n",
    "- Minimising RMSE and MSE leads to the same result since the value that minimises a function also minimizes the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae45d10-426f-4418-a846-0b552700de20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import necessary libraries for data wrangling and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import machine learning libraries and modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b80e5-d896-4097-b335-37b5522258aa",
   "metadata": {},
   "source": [
    "Let's use multiple-linear regression to predict the income of a person based of two key variables:\n",
    "- Age: Age of person\n",
    "- Experience: No. of years a user has worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfaf58c2-5d00-4e2d-b9c1-d378d7c5c3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>experience</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>30450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>35670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>31580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>40130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>47830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>41630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>41340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>37650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>40250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>45150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>27840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>46110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>36720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>34800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>51300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>38900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>58</td>\n",
       "      <td>17</td>\n",
       "      <td>63600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>30870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>44190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>48700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  experience  income\n",
       "0    25           1   30450\n",
       "1    30           3   35670\n",
       "2    47           2   31580\n",
       "3    32           5   40130\n",
       "4    43          10   47830\n",
       "5    51           7   41630\n",
       "6    28           5   41340\n",
       "7    33           4   37650\n",
       "8    37           5   40250\n",
       "9    39           8   45150\n",
       "10   29           1   27840\n",
       "11   47           9   46110\n",
       "12   54           5   36720\n",
       "13   51           4   34800\n",
       "14   44          12   51300\n",
       "15   41           6   38900\n",
       "16   58          17   63600\n",
       "17   23           1   30870\n",
       "18   44           9   44190\n",
       "19   37          10   48700"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load some data from kaggle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the directory you want to search in\n",
    "path = \"C:\\\\Users\\\\jaspe\\\\OneDrive\\\\Documents\"\n",
    "\n",
    "# Specify the name of the file you're looking for\n",
    "file_to_find = \"multiple_linear_regression_dataset.csv\"\n",
    "\n",
    "# Use os.listdir to get a list of all files in the directory\n",
    "for file in os.listdir(path):\n",
    "    if file == file_to_find:\n",
    "        # Now you can read the file as a CSV using pandas\n",
    "        data = pd.read_csv(os.path.join(path, file))\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57411143-09e7-480b-9144-6f132743f16b",
   "metadata": {},
   "source": [
    "For simplicty, let's just look at the relationship between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ccb2067-0398-4027-9a45-548f1edf5c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data[['age', 'experience']] #independent variable\n",
    "y = data['income'] #dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdc600-841f-4015-a92f-6670e35e9b87",
   "metadata": {},
   "source": [
    "To perform basic linear regression analysis, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fa5dfa-9237-4a99-b432-ade95b3e3e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression() #instantiate \n",
    "lr_fit = lr.fit(X, y) #fit/train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8c811-a308-47f2-9800-15f103744225",
   "metadata": {
    "tags": []
   },
   "source": [
    "To find the intercepts and coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed592611-43e0-4d99-909f-c82b54715efc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31261.689854101285, array([ -99.19535546, 2162.40419192]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_, lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b42e8d-88a6-488b-be0f-792b024ee8f1",
   "metadata": {},
   "source": [
    "From here, we can see that our model is now:\n",
    "\n",
    "\\begin{gather*}\n",
    "y_{pred}=-99.19x_{1}+2162.40x_{2}+31261.69\\\\\n",
    "\\end{gather*}\n",
    "\n",
    "\\begin{gather*}\n",
    "y_{pred}: predicted \\ income \\\\\n",
    "x_{1}: age \\ input feature \\\\\n",
    "x_{2}: experience \\ input feature\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb52ec-5cdc-41da-9c6d-7a905710754d",
   "metadata": {},
   "source": [
    "Let's predict the income of a person based of the model above using the following example:\n",
    "- Age = 23\n",
    "- Experience = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd710d6a-c2f1-4ed7-b581-476a4325860c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([35467.40925427])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = [[23,3]] #initialise age and experience as array\n",
    "\n",
    "y_pred = lr_fit.predict(X_test) #predict our income\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27dbf9-b17f-4b09-9667-4c251176fbf9",
   "metadata": {},
   "source": [
    "Therefore, our predicted income is $35,467 if an individual was 23 and his experience was 3 years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439c9d4-92e7-4420-acde-437f21298d2b",
   "metadata": {},
   "source": [
    "The computational time of LinearRegression() is $O(n^{2})$. Since the normal equation computes the psuedoinverse (a standard matrix factorization technique called SINGULAR VALUE DECOMPOSITION (SVD)), if you double the number of features in the model, you multiply the computational time by 4.\n",
    "\n",
    "Normal regression (least-squares) has a few issues with it:\n",
    "- It is not efficient to use normal linear regression if the no. of features in our dataset is less than the no. of trainin instances\n",
    "- If some features are considered redundant\n",
    "\n",
    "However, the computational complexity is linear with regards to:\n",
    "- No. of instancecs you want to make predictions on\n",
    "- No. of features\n",
    "\n",
    "Therefore, it can handle large trainingn sets provided that it can fit in your memory.\n",
    "\n",
    "Once your linear regression model is trained, it takes very little time to predict since the computational complexity is linear with regards to the no. of instances and no. of features e.g. if you increase the no. of instances (or no. of features) twice, it will take twice as long to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a4e6d-a1a7-4c4a-90c3-449c3c01289e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Gradient Descent</h1>\n",
    "\n",
    "Gradient Descent algorithms are responsible for helping the training rate of models by helping optimize and tweak parameters to minimize the cost function. \n",
    "\n",
    "- Gradient Descent takes the no. of steps $\\theta$ and the local gradient/steepness at that stepping point until it reaches a global minimum once the gradient becomes 0. \n",
    "- Gradient Descent tries to fill the stepping size with values (random initialization.\n",
    "- The goal is to minimize the cost function as much as possible (using MSE)\n",
    "- Step size is important to the learning rate:\n",
    "    - Step size is too big = may end up diverging from global minimum\n",
    "    - Step size is to small = will take a long time for the model to train due to low learning rates\n",
    "\n",
    "Sometimes, the cost function may not be perfectly concave (regular bowl) shaped and may contain ridges and bumps along the way, smaller dips along the cost function are known as local minimum.\n",
    "- If random initialization $\\theta$ starts on the left, it will reach a local minimum\n",
    "- If random initialization starts on the right, it will take a long time\n",
    "- If we stop early, we may never reach the global minimum\n",
    "\n",
    "The MSE cost function is convex in nature, meaning if we take two points from the function and run a line segment between them, it will it never intersect the curve -> no local minimums -> it's also a conitnous function.\n",
    "\n",
    "A few other key points:\n",
    "- Gradient descent can get very close the the global minimum (if we wait long enough and the learning rate isnt too high)\n",
    "\n",
    "Visually speaking, Gradient Descent algorithms approach the global minimum in a straight manner but if the data is scaled, the shape will become elongated and approach the global minimum at an orthogonal angle. \n",
    "\n",
    "<b>NOTE: YOU MUST SCALE YOUR FEATURES using StandardScaler() or else due to the magnitude and scale of the original dat, it could take a long time to converge.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd32e3a-64ec-43d2-8fff-2c499094de1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Batch Gradient Descent</h2>\n",
    "\n",
    "Batch gradient descent attempts to calculate the local gradient at each point of $\\theta$ bit by bit. Essentially we are taking the partial derivative at each point. \n",
    "\n",
    "Batch Gradient Descent has one main downfall:\n",
    "- Batch Gradient descent looks at the full training set of your data and hence it can take a very long time to train your models and can be terribly slow as the number of training instances increase.\n",
    "\n",
    "But...\n",
    "- Gradient descent algorithms can scale well as the no. of features increase, therefore training a linear regression model on data with large no. of features will be quicker with Gradient descent rather than using normal least squares.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0951e-2ca4-4989-9371-d24da8e77d00",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "X_b = np.c_[np.ones((20,1)), X]\n",
    "\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 20\n",
    "\n",
    "theta = np.random.randn(20,3)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "\n",
    "    gradients = 2/m*X_b.T.dot(X_b.dot(theta)-y)\n",
    "    \n",
    "    theta = theta - eta*gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a68241-29d9-4614-a19d-ffbde6bfdec5",
   "metadata": {},
   "source": [
    "If the cost function is convex (in most cases for MSE cost functions), Batch Gradient Descent with a fixed learning rate, it will eventually reach an optimal solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117f93f-66be-4e59-b842-21d754dcc152",
   "metadata": {},
   "source": [
    "<h2>Stochastic Gradient Descent</h2>\n",
    "\n",
    "Stochastic Gradient Descent randomly selects values of $\\theta$ rather than using the full training set at each step. This means that the gradient will be calculated at that point alone. \n",
    "- It works on a single instance rather the full instance each time\n",
    "- Can be used for larger training instances.\n",
    "- SGD can be good to get out of local minimums due to random \"jumping\" in error values\n",
    "\n",
    "Issue however:\n",
    "- Stochastic methods being random will gently decrease to until it reaches the minimum to which it will continue to \"bounce and fluctuate\" in error.\n",
    "- This makes it difficult to attain an optimal minimum\n",
    "- If the cost function is irregular, it can be easy for the SGD to reach global minimum.\n",
    "\n",
    "To solve the issue of not reaching the global minimum we can:\n",
    "- reduce the learning rate\n",
    "- make the first step sizes larger then slowly decrease it as $\\theta$ changes\n",
    "\n",
    "The function that determines the learning rate is the learning <b>schedule</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2728c-e564-41aa-9c05-52e7b75d3672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
